# CSV = "androbin_20.csv.zip"
#!/usr/bin/env python
# coding: utf-8

# Open csv with pandas:

# In[1]:

# 34700 2016-02-08
# 82567 2016-03-01
# 189099 2016-06-01
# 284547 2016-12-01
# 300267 2017-03-01
# 303960 2017-06-01
# 313371 2017-09-01
# 332065 2018-01-01
# 335344 2018-03-01
# 340780 2018-06-01
for TRAIN_NROWS in [25842]:
    # CSV location
    CSV = "androbin_20_final.csv.zip"


    # In[2]:


    COLS = ["resource.entry", "source.class.package", "manifest.permission", "manifest.activity", "manifest.action", "manifest.category", "manifest.feature", "source.method.name", "label", "meta.vt.date"]

    LABEL_COL = "label"

    TIME_COL = "meta.vt.date"


    # In[3]:
    import pandas as pd
    import numpy as np
    import scipy.sparse as sp
    # read data
    data = pd.read_csv(CSV,compression='zip',keep_default_na=False,usecols=COLS)#,nrows=10000)
    # get labels (0 = goodware, 1 = malware)
    labels = data[LABEL_COL]
    # remove not used columns
    UNUSED_COLUMNS = ["label", "meta.vt.date"]

    # Transform submission_date in a datetime column:

    # In[3]:


    # declare submission_date as date time
    data[TIME_COL] =  pd.to_datetime(data[TIME_COL])


    # Group data by months and create subsets to train and test:

    # In[4]:


    # initial_year = 2016
    # TRAIN_NROWS = 34700
    # TRAIN_NROWS = 5000
    dataTrain = data.head(TRAIN_NROWS)
    dataTest = data.iloc[TRAIN_NROWS:]
    # get years and month
    trainYears = dataTrain[TIME_COL].dt.year.values
    trainMonths = dataTrain[TIME_COL].dt.month.values
    testYears = dataTest[TIME_COL].dt.year.values
    testMonths = dataTest[TIME_COL].dt.month.values
    # get labels
    y_train = np.array(dataTrain[LABEL_COL])
    y_test = np.array(dataTest[LABEL_COL])
    # remove unused columns from dataTrain
    print(data.columns.values)
    for c in UNUSED_COLUMNS:
        del data[c]
        del dataTrain[c]
        del dataTest[c]
    # get values
    X_dataTrain = dataTrain.values
    X_dataTest = dataTest.values


    # Train TFIDF model for each column:

    # In[9]:


    def feature_extraction(X_dataTrain, X_dataTest, columns=None):
        from sklearn.feature_extraction.text import TfidfVectorizer
        # initialize X_train and X_test
        X_train = None
        X_test = None
        # save models used
        models = []
        # iterate over each column of X_dataTrain
        for i in range(X_dataTrain.shape[1]):
            try:
                # train feature word2vec using column i
                # if len(columns)>0:
                #     print(str(columns[i]))
                #     print
                # else:
                print("Column {}...".format(i))
                # get train and test data
                train_data = X_dataTrain[:,i]
                test_data = X_dataTest[:,i]
                # initialize and train model
                tfidf = TfidfVectorizer(max_features=100)
                tfidf.fit(train_data)
                vocab = list(tfidf.vocabulary_.keys())
                vocab.sort()
                print(vocab, flush=True)
                print
                # transform train and test texts to w2v mean
                train_tfidf = tfidf.transform(train_data)
                test_tfidf = tfidf.transform(test_data)
                # if first execution, save only features
                if X_train == None:
                    X_train = train_tfidf
                    X_test = test_tfidf
                # concatenate existing features
                else:
                    X_train = sp.hstack((X_train, train_tfidf), format='csr')
                    #np.concatenate((X_train, train_tfidf), axis=1)
                    X_test = sp.hstack((X_test, test_tfidf), format='csr')
                    #np.concatenate((X_test, test_tfidf), axis=1)
                # save model
                models.append(tfidf)
            except:
                print("Empty vocab for column {}".format(i))
                pass
        return(X_train, X_test)
    X_train, X_test = feature_extraction(X_dataTrain, X_dataTest, columns=data.columns.values)


    # Normalize data:

    # In[10]:


    # normalize features
    def normalization(X_train, X_test):
        from sklearn.preprocessing import MinMaxScaler
        from sklearn.preprocessing import MaxAbsScaler
        # scaler = MinMaxScaler()
        scaler = MaxAbsScaler()
        scaler.fit(X_train)
        XX_train = scaler.transform(X_train)
        XX_test = scaler.transform(X_test)
        return(XX_train, XX_test)
    XX_train, XX_test = normalization(X_train, X_test)


    # Initialize drift detection method and train base classifier:

    # In[11]:


    from skmultiflow.drift_detection import DDM, EDDM, ADWIN, KSWIN, PageHinkley
    from sklearn.ensemble import RandomForestClassifier
    # from skmultiflow.meta.adaptive_random_forests import AdaptiveRandomForest as Classifier
    from sklearn.linear_model import SGDClassifier as Classifier
    # initialize drift detection method
    drift = KSWIN(alpha=0.01, window_size=500, stat_size=125)
    # create base classifier trained with K samples
    # clf = Classifier(random_state=0, drift_detection_method=None, warning_detection_method=None)
    clf = Classifier(random_state=0)
    # clf2 = Classifier(random_state=0, drift_detection_method=None, warning_detection_method=None)
    clf2 = Classifier(random_state=0)
    print("Training classifier 1...")
    clf.partial_fit(XX_train,y_train, classes=[0,1])
    # print("Training classifier 2...")
    # clf2.partial_fit(XX_train,y_train, classes=[0,1])
    print("Ready for classification.")


    # Process data stream:

    # In[ ]:


    from sklearn.metrics import accuracy_score
    import math
    # drift points
    drifts = []
    # warning points
    warnings = []
    # flags for drift and warning
    DRIFT = False
    WARNING = False
    warning_data = []
    y_warning = []
    # accuracies
    accs = []
    # predictions
    pred = []
    true = []
    # hits
    hits = 0
    p = [] #vetor para erro prequencial
    s = []; #vetor para desvio padrao
    n = 1.0 #Conta amostras de cada conceito
    p.append(1.0) #erro prequential
    X_window = XX_train # window X
    y_window = y_train.tolist() # window y
    # iterate over stream
    for i in range(XX_test.shape[0]):
        # get batch samples
        sample_x = XX_test[i]
        sample_y = y_test[i]
        # add sample to window
        #X_window.append(sample_x) #= np.append(X_window, [sample_x], axis=0)
        X_window = sp.vstack((X_window, sample_x))
        y_window.append(sample_y) #= np.append(y_window, [sample_y])
        # predict data
        y_pred = clf.predict(sample_x)
        # save prediction and sample_y
        pred.append(y_pred)
        true.append(sample_y)
        # check if predicted is a hit
        if sample_y == y_pred:
            hits = hits + 1
            p.append(p[-1]-p[-1]/n);
        else:
            p.append(p[-1]+(1-p[-1])/n);
        s.append(math.sqrt(p[-1]*(1-p[-1])/n)) #calcula o desvio padrao
        n = n+1;
        # feed drift detector
        drift.add_element(int(sample_y == y_pred))
        # save metrics
        accs.append(float(hits)/float(i+1))
        # detect if warning or drift
    #     if drift.detected_warning_zone():
    #         if not WARNING:
    #             print("Warning in {}...".format(i+1))
    #             print("CLF Instances:", clf.instances_seen)
    #         # fit clf and clf2
    #         clf.partial_fit([sample_x],[sample_y])
    #         clf2.partial_fit([sample_x],[sample_y])
    #         warnings.append(i+1)
    #         # save data related to warning...
    #         warning_data.append(X_dataTest[i])
    #         y_warning.append(y_test[i])
    #         # print(X_dataTest.values[stream.sample_idx-1])
    #         WARNING = True
        if drift.detected_change():
            print('Change has been detected in {}.'.format(i))
            # print("info: ", drift.get_info())
            # print("width: ", drift.width)
            # print("width_t: ", drift.width_t)
            # print("total: ", drift.total)
            # print("n_detections: ", drift.n_detections)
            # print("get_change: ", drift.get_change())
            # print("estimation: ", drift.estimation)
            p.append(1.0)
            s.append(0.0);
            n = 1.0
            # save data related to drift (warning yet)
            drifts.append(i)
            # update window data and labels
            X_window = X_window[-(X_window.shape[0] - drift.window_size):]
            y_window = y_window[-(len(y_window) - drift.window_size):]
            print("X_window: ", X_window.shape[0])#.shape)
            print("y_window: ", len(y_window)) #.shape)
            # change classifiers
            clf2.partial_fit(X_window, y_window, classes=[0,1])
            clf = clf2
            # clf2 = AdaptiveRandomForest(random_state=0, drift_detection_method=None, warning_detection_method=None)
            clf2 = Classifier(random_state=0)
            # print("CLF Instances:", clf.instances_seen)
            # reset drifts information
            drift.reset()
            WARNING = False
            warning_data = []
            y_warning = []
        else:
            WARNING = False
            warning_data = []
            y_warning = []
            # clf2 = AdaptiveRandomForest(random_state=0, drift_detection_method=None, warning_detection_method=None)
            clf2 = Classifier(random_state=0)
            clf.partial_fit(sample_x,[sample_y], classes=[0,1])
    print("Drifts: ", drifts)
    print("AVG Accuracy: ", np.mean(accs))


    # Plot accuracies and drift:

    # In[11]:


    import matplotlib.pyplot as plt
    from matplotlib.font_manager import FontProperties
    font = FontProperties()
    font.set_weight("bold")
    plt.plot(p)
    pre = []
    for d in drifts:
        pre.append(p[d])
        plt.axvline(d, color="red", ymin=0.08, ymax=0.92, linewidth=1)
    plt.plot(drifts,pre, 'ro')
    # plot years information
    last_year = testYears[0]
    last_i = 0
    plt.axvline(0,color="gray",linewidth=2)
    plt.text(1000,1.03,last_year, color="gray", fontproperties=font)
    for i, y in enumerate(testYears):
        if y!=last_year:
            if i > last_i+2000:
                plt.axvline(i,color="gray",linewidth=2)
                plt.text(i+1000,1.03,y, color="gray", fontproperties=font)
                last_year = y
                last_i = i
    last = testYears[len(testYears)-1]
    if last_year != last:
        plt.text(i+1000,0.95,last, color="gray", fontproperties=font)
    plt.xlim(-12500,len(testYears)+12500)
    plt.ylim(-0.1,1.1)
    plt.xlabel("Number of Examples")
    plt.ylabel("Prequential Error")
    # plt.grid(True)
    plt.title("TFIDF & SGD - KSWIN with Update")
    plt.savefig("figures/exp_4_kswin_update_{}.pdf".format(TRAIN_NROWS))
    plt.savefig("figures/exp_4_kswin_update_{}.png".format(TRAIN_NROWS))
    plt.show()


    # Print confusion matrix:

    # In[12]:


    from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix
    # classes names
    print("Accuracy: ", accuracy_score(true, pred))
    print("Overall F1Score: ", f1_score(true, pred))
    print("Overall Recall: ", recall_score(true, pred))
    print("Overall Precision: ", precision_score(true, pred))
    print("Confusion Matrix:")
    print(confusion_matrix(true,pred), flush=True)


    # In[ ]:


    print("Drifts: ", drifts)

    f = open("results/exp_4_kswin_update_{}.txt".format(TRAIN_NROWS), "a")
    f.write("Accuracy: " + str(accuracy_score(true, pred)) + "\n")
    f.write("Overall F1Score: " + str(f1_score(true, pred)) + "\n")
    f.write("Overall Recall: " + str(recall_score(true, pred)) + "\n")
    f.write("Overall Precision: " + str(precision_score(true, pred)) + "\n")
    f.write("Confusion Matrix:\n")
    # f.write(confusion_matrix(true,pred))
    # In[ ]:
    f.write("Drifts: " + str(drifts) + "\n")
    f.write("# of Drifts: " + str(len(drifts)))
    f.close()
